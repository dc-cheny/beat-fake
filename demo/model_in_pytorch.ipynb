{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "# See github.com/timesler/facenet-pytorch:\n",
    "from facenet_pytorch import InceptionResnetV1, extract_face\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Running on device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face detector\n",
    "mtcnn = MTCNN(margin=14, keep_all=True, factor=0.5, device=device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading parameters (1/2)\n",
      "Downloading parameters (2/2)\n"
     ]
    }
   ],
   "source": [
    "# Load facial recognition model\n",
    "resnet = InceptionResnetV1(pretrained='vggface2', device=device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionPipeline:\n",
    "    \"\"\"Pipeline class for detecting faces in the frames of a video file.\"\"\"\n",
    "    \n",
    "    def __init__(self, detector, n_frames=None, batch_size=10, resize=None):\n",
    "        \"\"\"Constructor for DetectionPipeline class.\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            n_frames {int} -- Total number of frames to load. These will be evenly spaced\n",
    "                throughout the video. If not specified (i.e., None), all frames will be loaded.\n",
    "                (default: {None})\n",
    "            batch_size {int} -- Batch size to use with MTCNN face detector. (default: {32})\n",
    "            resize {float} -- Fraction by which to resize frames from original prior to face\n",
    "                detection. A value less than 1 results in downsampling and a value greater than\n",
    "                1 result in upsampling. (default: {None})\n",
    "        \"\"\"\n",
    "        self.detector = detector\n",
    "        self.n_frames = n_frames\n",
    "        self.batch_size = batch_size\n",
    "        self.resize = resize\n",
    "    \n",
    "    def __call__(self, filename):\n",
    "        \"\"\"Load frames from an MP4 video and detect faces.\n",
    "\n",
    "        Arguments:\n",
    "            filename {str} -- Path to video.\n",
    "        \"\"\"\n",
    "        # Create video reader and find length\n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Pick 'n_frames' evenly spaced frames to sample\n",
    "        if self.n_frames is None:\n",
    "            sample = np.arange(0, v_len)\n",
    "        else:\n",
    "            sample = np.linspace(0, v_len - 1, self.n_frames).astype(int)\n",
    "\n",
    "        # Loop through frames\n",
    "        faces = []\n",
    "        frames = []\n",
    "        for j in range(v_len):\n",
    "            success = v_cap.grab()\n",
    "            if j in sample:\n",
    "                # Load frame\n",
    "                success, frame = v_cap.retrieve()\n",
    "                if not success:\n",
    "                    continue\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = Image.fromarray(frame)\n",
    "                \n",
    "                # Resize frame to desired size\n",
    "                if self.resize is not None:\n",
    "                    frame = frame.resize([int(d * self.resize) for d in frame.size])\n",
    "                frames.append(frame)\n",
    "\n",
    "                # When batch is full, detect faces and reset frame list\n",
    "                if len(frames) % self.batch_size == 0 or j == sample[-1]:\n",
    "                    faces.extend(self.detector(frames))\n",
    "                    frames = []\n",
    "\n",
    "        v_cap.release()\n",
    "\n",
    "        return faces    \n",
    "\n",
    "\n",
    "def process_faces(faces, resnet):\n",
    "    # Filter out frames without faces\n",
    "    faces = [f for f in faces if f is not None]\n",
    "    faces = torch.cat(faces).to(device)\n",
    "\n",
    "    # Generate facial feature vectors using a pretrained model\n",
    "    embeddings = resnet(faces)\n",
    "\n",
    "    # Calculate centroid for video and distance of each face's feature vector from centroid\n",
    "    centroid = embeddings.mean(dim=0)\n",
    "    x = (embeddings - centroid).norm(dim=1).cpu().numpy()\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Define face detection pipeline\n",
    "detection_pipeline = DetectionPipeline(detector=mtcnn, n_frames=30, batch_size=10, resize=0.25)\n",
    "\n",
    "# Get all test videos\n",
    "filenames = glob.glob('../data/aassnaulhq.mp4')\n",
    "\n",
    "X = []\n",
    "start = time.time()\n",
    "n_processed = 0\n",
    "with torch.no_grad():\n",
    "    for i, filename in enumerate(filenames):\n",
    "        try:\n",
    "            # Load frames and find faces\n",
    "            faces = detection_pipeline(filename)\n",
    "            \n",
    "            # Calculate embeddings\n",
    "            X.append(process_faces(faces, resnet))\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\nStopped.')\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            X.append(None)\n",
    "        \n",
    "        n_processed += len(faces)\n",
    "        print(f'Frames per second (load+detect+embed): {n_processed / (time.time() - start):6.3}\\r', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=resnet(torch.cat(faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 3, 160, 160])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(faces).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0024,  0.0218, -0.0622,  ..., -0.0197, -0.0586, -0.0048],\n",
       "        [ 0.0265,  0.0207, -0.0502,  ..., -0.0311, -0.0177, -0.0092],\n",
       "        [ 0.0323,  0.0007, -0.0613,  ..., -0.0248, -0.0363, -0.0045],\n",
       "        ...,\n",
       "        [ 0.0036,  0.0707, -0.0460,  ...,  0.0056, -0.0603,  0.0092],\n",
       "        [ 0.0360,  0.0182, -0.0438,  ..., -0.0305,  0.0057,  0.0370],\n",
       "        [ 0.0186, -0.0229, -0.0828,  ...,  0.0175, -0.0572, -0.0325]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 512])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 512])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a-a.mean(dim=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a-a.mean(dim=0)).norm(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grab True\n",
      "retrieve True\n",
      "tensor([[[[-0.3477, -0.3477, -0.3398,  ..., -0.4023, -0.4023, -0.4023],\n",
      "          [-0.3477, -0.3477, -0.3477,  ..., -0.4023, -0.4023, -0.4023],\n",
      "          [-0.3555, -0.3555, -0.3555,  ..., -0.4102, -0.4102, -0.4102],\n",
      "          ...,\n",
      "          [-0.7695, -0.7695, -0.7695,  ..., -0.8477, -0.8711, -0.8789],\n",
      "          [-0.7695, -0.7695, -0.7695,  ..., -0.8555, -0.8867, -0.8945],\n",
      "          [-0.7695, -0.7695, -0.7695,  ..., -0.8633, -0.8945, -0.9023]],\n",
      "\n",
      "         [[-0.4336, -0.4336, -0.4414,  ..., -0.4883, -0.4883, -0.4883],\n",
      "          [-0.4336, -0.4336, -0.4414,  ..., -0.4883, -0.4883, -0.4883],\n",
      "          [-0.4258, -0.4258, -0.4336,  ..., -0.4805, -0.4805, -0.4805],\n",
      "          ...,\n",
      "          [-0.8711, -0.8711, -0.8711,  ..., -0.9336, -0.9570, -0.9648],\n",
      "          [-0.8711, -0.8711, -0.8711,  ..., -0.9492, -0.9648, -0.9727],\n",
      "          [-0.8711, -0.8711, -0.8711,  ..., -0.9570, -0.9727, -0.9805]],\n",
      "\n",
      "         [[-0.4883, -0.4883, -0.4961,  ..., -0.5273, -0.5273, -0.5273],\n",
      "          [-0.4883, -0.4883, -0.4961,  ..., -0.5195, -0.5195, -0.5195],\n",
      "          [-0.4883, -0.4883, -0.4961,  ..., -0.5117, -0.5117, -0.5117],\n",
      "          ...,\n",
      "          [-0.9102, -0.9102, -0.9180,  ..., -0.9414, -0.9648, -0.9727],\n",
      "          [-0.9102, -0.9102, -0.9180,  ..., -0.9492, -0.9727, -0.9805],\n",
      "          [-0.9102, -0.9102, -0.9180,  ..., -0.9570, -0.9805, -0.9883]]]])\n"
     ]
    }
   ],
   "source": [
    "v_cap = cv2.VideoCapture(filename)\n",
    "v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "success = v_cap.grab()\n",
    "print('grab',success)\n",
    "success, frame = v_cap.retrieve()\n",
    "print('retrieve',success)\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "frame = Image.fromarray(frame)\n",
    "frame = frame.resize([int(d * 0.5) for d in frame.size])\n",
    "face = mtcnn(frame)\n",
    "print(face)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
